{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b550bd1-9770-4d18-b5b3-77045fdd2fe0",
   "metadata": {},
   "source": [
    "# Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbeb1e-8b9a-4610-8da1-e586efd364b1",
   "metadata": {},
   "source": [
    "**Web Scraping** is the process of programmatically extracting data from a web interface designed for human interaction. Unfortunately, web scraping is hard to generalize and will usually require creating a bespoke scraper program for each platform. This requires digging into a site's sources to see how its data are organized. Python has a number of popular libraries<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) available that are useful for web scraping, but the ones we are going to use for this class are `requests` for making http requests, and `BeautifulSoup` for parsing html.\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Others include Scrapy and Selenium (a website testing library that can drive an actual browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31768b94-6dd3-4952-a29c-475da0ecdb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad82ba-9134-4378-a170-7d938bd9ac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04133476-978a-45ba-9089-9df10e489537",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "**HTML** (HyperText Markup Language) is the primary **markup language** used for websites. Markup languages add formatting and/or structure to (generally) textual data for the purposes of display or processing. You only really need a basic familiarity with HTML to get started webscraping. The basic mechanism HTML uses for adding structure and formatting to documents is the \"tag\". A tag consists of some keyword and optional attributes surrounded by angle brackets, i.e. \"<\" and \">\". Many tags have \"open\" and \"close\" variants, with the close variant starting with \"</\". A tag and its contents is commonly referred to as an \"element\".\n",
    "\n",
    "- HTML documents have a head and body section, denoted by tags of the same name. The head section contains a title for the document, various metadata, scripts, stylesheets, etc, which pertain to the document as a whole. The body contains the actual content of the document.\n",
    "- Markdown was conceived as a more human-friendly way to format plain text documents in a way that could easily be translated into HTML for web publishing. As a result, there is a close relationship between Markdown formatting and HTML elements. If you look at markdownguide.org, you will find most of the equivalent HTML used to render the markdown element.\n",
    "- We will mostly be focused on links and tables. We've already looked at tables. Links use the \"<a>\" tag (for anchor), and has an attribute \"href\" for the actual http reference or url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f58c8c-f7b3-4d9a-b6c5-70aa22a38b24",
   "metadata": {},
   "source": [
    "## HTTP\n",
    "\n",
    "The **HyperText Transport Protocol** is the network protocol used to retreive html documents and associated files from a server. We won't get into the details of the protocol, but you need to be aware of two concepts:\n",
    "1. Request methods. The most common request type is \"GET\", but there are several others. \"POST\" is used to send information to a server (for example, form data).\n",
    "2. Response status codes. If everything is correct, you will get a \"200\" status code. Status codes in the 300 range indicate some sort of redirection. Status codes in the 400 range indicate an apparent client error (including 404 file not found). Status codes in the 500 range indicate a server error.\n",
    "\n",
    "The wikipedia article can be a helpful reference: https://en.wikipedia.org/wiki/HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ab7c5-8f48-456f-b74f-4b32a6ba8c86",
   "metadata": {},
   "source": [
    "## Avoiding Getting Blocked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4238c0-df0f-4e19-97d0-61cf436d3cc2",
   "metadata": {},
   "source": [
    "### Rate Limiting\n",
    "\n",
    "Many websites have some level of protection against automated access. This is because, at the very least, because automated access may tie up most of the resources available to the website's server, effectively denying access to other users. It is always a good idea to build some way to rate-limit your own requests, or the website you are scraping may do it for you, if not outright block the IP you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39256375-974e-44c2-ab7b-193ab0b7da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "DEFAULT_SLEEP = 2.0\n",
    "SIGMA = 0.5\n",
    "\n",
    "HEADERS = {}\n",
    "\n",
    "def get(url: str) -> requests.Response:\n",
    "    time.sleep(random.gauss(DEFAULT_SLEEP, SIGMA))\n",
    "    return requests.get(url, headers=HEADERS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2564505-1299-4444-a77b-9314b1c97964",
   "metadata": {},
   "source": [
    "### User Agent\n",
    "\n",
    "The **User Agent** http header identifies the software being used to make the http request. It can be a good idea to set this to a value used by an actual browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24d229cc-29ff-4e88-8641-69b774540459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Agent from Chrome Browser on Win 10/11\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f269050-b0d2-4b49-a017-6addf108d856",
   "metadata": {},
   "source": [
    "### Proxy Servers\n",
    "\n",
    "It can also be useful to make use of a proxy server service. A proxy server allows you to direct webtraffic from your computer through another computer on the Internet. Proxy server services exist that allow you to use multiple IP addresses for making your web scrapping requests. This is a good way to avoid having your one and only IP address blocked, but does cost money."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6839e1c-5392-4df6-a326-1cc250af5de8",
   "metadata": {},
   "source": [
    "## Keeping Track of Our Progress\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c81be-7b4d-4558-8ffc-dbe914a95d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
