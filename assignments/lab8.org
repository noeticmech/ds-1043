#+title: Lab 8: In Summary
#+author:
#+date:  22 October 2024
:export:
#+latex_class: tufte-handout
#+options: toc:nil
#+latex_compiler: xelatex
#+latex_header: \usepackage[final]{microtype}
#+latex_header: \usepackage{fontspec}
#+latex_header: \setmainfont{Gentium Plus}
#+latex_header: \setmonofont[Scale=0.8]{Maple Mono NF}
#+latex_header: \renewcommand\allcapsspacing[1]{{\addfontfeature{LetterSpace=15}#1}}
#+latex_header: \renewcommand\smallcapsspacing[1]{{\addfontfeature{LetterSpace=10}#1}}
#+latex_header: \usepackage{enumitem}
#+latex_header: \setlist{nosep}
#+property: header-args :eval no-export
:end:

* Instructions
1. If you haven't already, clone the project at [[https://github.com/noeticmech/summarize]]. Allow PyCharm to install dependencies when prompted. If you have already cloned the project, please update your clone of the project by right-clicking on Remote > Origin > Master on the left-hand side of the version control pane, and selecting "Pull into 'master' Using Merge".
2. After you have done this, you should create your own "summarize" repository on GitHub, and point Remote > Origin to the URL of your repository by right-clicking on Remote and selecting "Manage Remotes". You can now push the project files (and your future changes) to your GitHub repository.
3. Note that you will need to download some data for ~nltk~ to tokenize words and sentences. You can do this in the python console by importing ~nltk~ and then calling ~nltk.download('punkt_tab')~.
4. Submit the URL of your repository and the tag for the release/commit you wish to be graded on Harvey.

* Term Frequency - Inverse Document Frequency (tf-idf)

#+begin_quote
In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a measure of importance of a word to a document in a collection or corpus, adjusted for the fact that some words appear more frequently in general. Like the bag-of-words model, it models a document as a multiset of words, without word order. It is a refinement over the simple bag-of-words model, by allowing the weight of words to depend on the rest of the corpus.

It was often used as a weighting factor in searches of information retrieval, text mining, and user modeling. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries used tf–idf. Variations of the tf–idf weighting scheme were often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.

One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.

-- Summary of tf-idf from Wikipedia
#+end_quote

TF-IDF is a way to generate weights words for various calculations based on the apparent importance of the word within a given set of documents (e.g. a corpus).

To use tf-idf, we need to inspect each document in our set of documents and gather the following data:

1. Creating a matrix or table of documents on one axis and terms on another. The values in this table will be the number of instances of the term within that document, divided by the total terms in the document. This is the /Term Frequency/ part of tf-idf.
2. Creating vector of terms, where the values are the total number of documents in the corpus divided by the number of documents in which the term occurs. This is the /Inverse Document Frequency/ part of tf-idf. For large sets of documents, it is often necessary to logarithmically scale the idf term to prevent this term from unduly dominating the weight.
3. The final weight for each term is the product of these two factors for each term in a given document in a given corpus.
4. Documents can be ranked based on the tf-idf of the terms they contain. A straightforward way of doing this would be to do an weighted average of all terms in the document.

* Summarize

We are going to use tf-idf to summarize a document by identifying the most important sentences in the document. In this case, we will treat each sentence in the document as a "document" and the document as a whole as the "corpus". You will need to adjust or implement four functions in the ~summarize.py~ program.

- ~clean_text~ creates a list of terms (including duplicates, which are needed to calculate tf-idf) for each sentence in the text. You should add some additional code to clean up the set of terms, removing non-word tokens for example.
- ~calculate_tf~ calculates the /tf/ for terms in each sentence of the text and returns a data structure containing these values. You should implement this function.
- ~calculate_idf~ calculates the /idf/ for each term and returns a data structure containing these values. You should implement this function.
- ~score_sentences~ creates a score for each sentence based on the tf-idf weights of the terms it contains relative to the size of the sentence. You should implement this function.

  Outside of the core calculation of tf-idf, there are lots of things that could be adjusted one way or the other, including things like different methods or criteria for inclusion in the summary, filtering terms before applying tf-idf for very common English words, etc. You should experiment with some of these matters of implementation and think about ways you could potentially change them for a better summary, or even how you might evaluate the quality of a summary. This will form part of the basis for the accompanying *Project 1*.
